{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7e33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SISTEMA DE DIAGN√ìSTICO M√âDICO - STACKING ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "[1/6] Carregando dataset...\n",
      "   ‚úì Dataset carregado: 246926 amostras, 378 colunas\n",
      "   ‚úì N√∫mero de doen√ßas √∫nicas: 754\n",
      "   ‚úì N√∫mero de sintomas: 377\n",
      "\n",
      "   Classes de doen√ßas codificadas: 0 a 753\n",
      "\n",
      "[2/6] Dividindo dados em treino e teste...\n",
      "   ‚úì Treino: 197540 amostras\n",
      "   ‚úì Teste: 49386 amostras\n",
      "\n",
      "[3/6] Configurando Stacking Ensemble...\n",
      "   ‚úì Base Learners:\n",
      "      - Random Forest (100 √°rvores)\n",
      "      - Gradient Boosting (100 √°rvores)\n",
      "      - Naive Bayes\n",
      "   ‚úì Meta Learner: Logistic Regression\n",
      "   ‚úì Cross-Validation: 5-fold Stratified\n",
      "\n",
      "[4/6] Treinando modelo Stacking...\n",
      "   (Isso pode levar alguns minutos...)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAMENTO E PR√â-PROCESSAMENTO DOS DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SISTEMA DE DIAGN√ìSTICO M√âDICO - STACKING ENSEMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "print(\"\\n[1/6] Carregando dataset...\")\n",
    "df = pd.read_csv('Final_Augmented_dataset_Diseases_and_Symptoms.csv')\n",
    "\n",
    "# Remover doen√ßas raras (ocorr√™ncia √∫nica)\n",
    "df = df[df['diseases'].isin(df['diseases'].value_counts()[df['diseases'].value_counts() > 1].index)]\n",
    "\n",
    "print(f\"   ‚úì Dataset carregado: {df.shape[0]} amostras, {df.shape[1]} colunas\")\n",
    "print(f\"   ‚úì N√∫mero de doen√ßas √∫nicas: {df['diseases'].nunique()}\")\n",
    "print(f\"   ‚úì N√∫mero de sintomas: {df.shape[1] - 1}\")\n",
    "\n",
    "# Separar features e target\n",
    "X = df.drop('diseases', axis=1)\n",
    "y = df['diseases']\n",
    "\n",
    "# Codificar labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\n   Classes de doen√ßas codificadas: 0 a {len(label_encoder.classes_) - 1}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DIVIS√ÉO DOS DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[2/6] Dividindo dados em treino e teste...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"   ‚úì Teste: {X_test.shape[0]} amostras\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DEFINI√á√ÉO DO MODELO STACKING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[3/6] Configurando Stacking Ensemble...\")\n",
    "\n",
    "# Base learners (modelos de primeira camada)\n",
    "base_learners = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt',\n",
    "        verbose=1  # mostra progresso da Random Forest\n",
    "    )),\n",
    "    ('gb', GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbose=1  # mostra progresso do Gradient Boosting\n",
    "    )),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "\n",
    "# Meta learner (modelo de segunda camada)\n",
    "meta_learner = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "# Criar Stacking Classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"   ‚úì Base Learners:\")\n",
    "print(\"      - Random Forest (100 √°rvores)\")\n",
    "print(\"      - Gradient Boosting (100 √°rvores)\")\n",
    "print(\"      - Naive Bayes\")\n",
    "print(\"   ‚úì Meta Learner: Logistic Regression\")\n",
    "print(\"   ‚úì Cross-Validation: 5-fold Stratified\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TREINAMENTO DO MODELO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[4/6] Treinando modelo Stacking...\")\n",
    "print(\"   (Isso pode levar alguns minutos...)\\n\")\n",
    "\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n   ‚úì Treinamento conclu√≠do!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. AVALIA√á√ÉO DO MODELO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[5/6] Avaliando modelo...\")\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred_train = stacking_model.predict(X_train)\n",
    "y_pred_test = stacking_model.predict(X_test)\n",
    "\n",
    "# Acur√°cia\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\n   ‚úì Acur√°cia no treino: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"   ‚úì Acur√°cia no teste:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Relat√≥rio de classifica√ß√£o (resumido)\n",
    "print(\"\\n   Relat√≥rio de Classifica√ß√£o (Teste):\")\n",
    "print(\"   \" + \"-\" * 76)\n",
    "report = classification_report(y_test, y_pred_test, \n",
    "                              target_names=label_encoder.classes_,\n",
    "                              output_dict=True)\n",
    "\n",
    "# Mostrar m√©tricas m√©dias\n",
    "print(f\"   Precis√£o m√©dia:  {report['weighted avg']['precision']:.4f}\")\n",
    "print(f\"   Recall m√©dio:    {report['weighted avg']['recall']:.4f}\")\n",
    "print(f\"   F1-Score m√©dio:  {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. SALVAR MODELO E ARTEFATOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[6/6] Salvando modelo e artefatos...\")\n",
    "\n",
    "# Salvar modelo\n",
    "joblib.dump(stacking_model, 'stacking_disease_model.pkl')\n",
    "print(\"   ‚úì Modelo salvo: stacking_disease_model.pkl\")\n",
    "\n",
    "# Salvar label encoder\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"   ‚úì Label encoder salvo: label_encoder.pkl\")\n",
    "\n",
    "# Salvar lista de features\n",
    "feature_names = X.columns.tolist()\n",
    "with open('feature_names.json', 'w') as f:\n",
    "    json.dump(feature_names, f)\n",
    "print(\"   ‚úì Features salvas: feature_names.json\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. FUN√á√ÉO DE PREDI√á√ÉO COM JSON\n",
    "# ==============================================================================\n",
    "\n",
    "def predict_disease_from_json(json_input):\n",
    "    \"\"\"\n",
    "    Prediz doen√ßa a partir de um JSON de sintomas.\n",
    "    \n",
    "    Args:\n",
    "        json_input (dict ou str): JSON com sintomas (1 = presente, 0 = ausente)\n",
    "                                  Sintomas n√£o fornecidos s√£o considerados 0\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dicion√°rio com predi√ß√£o e probabilidades\n",
    "    \"\"\"\n",
    "    # Carregar artefatos\n",
    "    model = joblib.load('stacking_disease_model.pkl')\n",
    "    encoder = joblib.load('label_encoder.pkl')\n",
    "    with open('feature_names.json', 'r') as f:\n",
    "        features = json.load(f)\n",
    "    \n",
    "    # Parse JSON se for string\n",
    "    if isinstance(json_input, str):\n",
    "        json_input = json.loads(json_input)\n",
    "    \n",
    "    # Criar vetor de features (default = 0)\n",
    "    feature_vector = np.zeros(len(features))\n",
    "    \n",
    "    # Preencher sintomas fornecidos\n",
    "    for symptom, value in json_input.items():\n",
    "        if symptom in features:\n",
    "            idx = features.index(symptom)\n",
    "            feature_vector[idx] = int(value)\n",
    "    \n",
    "    # Reshape para predi√ß√£o\n",
    "    feature_vector = feature_vector.reshape(1, -1)\n",
    "    \n",
    "    # Predi√ß√£o\n",
    "    prediction = model.predict(feature_vector)[0]\n",
    "    probabilities = model.predict_proba(feature_vector)[0]\n",
    "    \n",
    "    # Decodificar doen√ßa\n",
    "    disease = encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    # Top 5 doen√ßas mais prov√°veis\n",
    "    top_5_indices = np.argsort(probabilities)[-5:][::-1]\n",
    "    top_5_diseases = [\n",
    "        {\n",
    "            'disease': encoder.inverse_transform([idx])[0],\n",
    "            'probability': float(probabilities[idx])\n",
    "        }\n",
    "        for idx in top_5_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'predicted_disease': disease,\n",
    "        'confidence': float(probabilities[prediction]),\n",
    "        'top_5_predictions': top_5_diseases\n",
    "    }\n",
    "\n",
    "print(\"\\n   ‚úì Fun√ß√£o de predi√ß√£o criada: predict_disease_from_json()\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. EXEMPLO DE USO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXEMPLO DE PREDI√á√ÉO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Exemplo de JSON com sintomas\n",
    "example_json = {\n",
    "    \"fever\": 1,\n",
    "    \"cough\": 1,\n",
    "    \"fatigue\": 1,\n",
    "    \"headache\": 1,\n",
    "    \"shortness of breath\": 1\n",
    "}\n",
    "\n",
    "print(\"\\nSintomas de entrada (JSON):\")\n",
    "print(json.dumps(example_json, indent=2))\n",
    "\n",
    "# Realizar predi√ß√£o\n",
    "result = predict_disease_from_json(example_json)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RESULTADO DA PREDI√á√ÉO:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\nDoen√ßa Prevista: {result['predicted_disease']}\")\n",
    "print(f\"Confian√ßa: {result['confidence']:.4f} ({result['confidence']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTop 5 Diagn√≥sticos Mais Prov√°veis:\")\n",
    "for i, pred in enumerate(result['top_5_predictions'], 1):\n",
    "    print(f\"   {i}. {pred['disease']}: {pred['probability']:.4f} ({pred['probability']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SISTEMA PRONTO PARA USO!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPara fazer novas predi√ß√µes, use:\")\n",
    "print(\"   result = predict_disease_from_json(seu_json)\")\n",
    "print(\"\\nExemplo de JSON vazio (todos sintomas = 0):\")\n",
    "print(\"   result = predict_disease_from_json({})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==============================================================================\n",
    "# INFORMA√á√ïES SOBRE MEM√ìRIA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nüìä Uso de Mem√≥ria:\")\n",
    "print(f\"   - Tamanho do modelo em disco: ~{joblib.dump(stacking_model, 'temp.pkl')} bytes\")\n",
    "import os\n",
    "if os.path.exists('temp.pkl'):\n",
    "    size_mb = os.path.getsize('temp.pkl') / (1024 * 1024)\n",
    "    print(f\"   - Tamanho aproximado: {size_mb:.2f} MB\")\n",
    "    os.remove('temp.pkl')\n",
    "\n",
    "print(\"\\n‚úÖ Sistema otimizado para uso consistente e eficiente de mem√≥ria!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
