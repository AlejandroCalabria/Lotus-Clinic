{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7e33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SISTEMA DE DIAGNÓSTICO MÉDICO - STACKING ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "[1/6] Carregando dataset...\n",
      "   ✓ Dataset carregado: 246926 amostras, 378 colunas\n",
      "   ✓ Número de doenças únicas: 754\n",
      "   ✓ Número de sintomas: 377\n",
      "\n",
      "   Classes de doenças codificadas: 0 a 753\n",
      "\n",
      "[2/6] Dividindo dados em treino e teste...\n",
      "   ✓ Treino: 197540 amostras\n",
      "   ✓ Teste: 49386 amostras\n",
      "\n",
      "[3/6] Configurando Stacking Ensemble...\n",
      "   ✓ Base Learners:\n",
      "      - Random Forest (100 árvores)\n",
      "      - Gradient Boosting (100 árvores)\n",
      "      - Naive Bayes\n",
      "   ✓ Meta Learner: Logistic Regression\n",
      "   ✓ Cross-Validation: 5-fold Stratified\n",
      "\n",
      "[4/6] Treinando modelo Stacking...\n",
      "   (Isso pode levar alguns minutos...)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAMENTO E PRÉ-PROCESSAMENTO DOS DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SISTEMA DE DIAGNÓSTICO MÉDICO - STACKING ENSEMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "print(\"\\n[1/6] Carregando dataset...\")\n",
    "df = pd.read_csv('Final_Augmented_dataset_Diseases_and_Symptoms.csv')\n",
    "\n",
    "# Remover doenças raras (ocorrência única)\n",
    "df = df[df['diseases'].isin(df['diseases'].value_counts()[df['diseases'].value_counts() > 1].index)]\n",
    "\n",
    "print(f\"   ✓ Dataset carregado: {df.shape[0]} amostras, {df.shape[1]} colunas\")\n",
    "print(f\"   ✓ Número de doenças únicas: {df['diseases'].nunique()}\")\n",
    "print(f\"   ✓ Número de sintomas: {df.shape[1] - 1}\")\n",
    "\n",
    "# Separar features e target\n",
    "X = df.drop('diseases', axis=1)\n",
    "y = df['diseases']\n",
    "\n",
    "# Codificar labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\n   Classes de doenças codificadas: 0 a {len(label_encoder.classes_) - 1}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DIVISÃO DOS DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[2/6] Dividindo dados em treino e teste...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"   ✓ Treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"   ✓ Teste: {X_test.shape[0]} amostras\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DEFINIÇÃO DO MODELO STACKING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[3/6] Configurando Stacking Ensemble...\")\n",
    "\n",
    "# Base learners (modelos de primeira camada)\n",
    "base_learners = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt',\n",
    "        verbose=1  # mostra progresso da Random Forest\n",
    "    )),\n",
    "    ('gb', GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbose=1  # mostra progresso do Gradient Boosting\n",
    "    )),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "\n",
    "# Meta learner (modelo de segunda camada)\n",
    "meta_learner = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "# Criar Stacking Classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"   ✓ Base Learners:\")\n",
    "print(\"      - Random Forest (100 árvores)\")\n",
    "print(\"      - Gradient Boosting (100 árvores)\")\n",
    "print(\"      - Naive Bayes\")\n",
    "print(\"   ✓ Meta Learner: Logistic Regression\")\n",
    "print(\"   ✓ Cross-Validation: 5-fold Stratified\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TREINAMENTO DO MODELO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[4/6] Treinando modelo Stacking...\")\n",
    "print(\"   (Isso pode levar alguns minutos...)\\n\")\n",
    "\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n   ✓ Treinamento concluído!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. AVALIAÇÃO DO MODELO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[5/6] Avaliando modelo...\")\n",
    "\n",
    "# Predições\n",
    "y_pred_train = stacking_model.predict(X_train)\n",
    "y_pred_test = stacking_model.predict(X_test)\n",
    "\n",
    "# Acurácia\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\n   ✓ Acurácia no treino: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"   ✓ Acurácia no teste:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Relatório de classificação (resumido)\n",
    "print(\"\\n   Relatório de Classificação (Teste):\")\n",
    "print(\"   \" + \"-\" * 76)\n",
    "report = classification_report(y_test, y_pred_test, \n",
    "                              target_names=label_encoder.classes_,\n",
    "                              output_dict=True)\n",
    "\n",
    "# Mostrar métricas médias\n",
    "print(f\"   Precisão média:  {report['weighted avg']['precision']:.4f}\")\n",
    "print(f\"   Recall médio:    {report['weighted avg']['recall']:.4f}\")\n",
    "print(f\"   F1-Score médio:  {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. SALVAR MODELO E ARTEFATOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[6/6] Salvando modelo e artefatos...\")\n",
    "\n",
    "# Salvar modelo\n",
    "joblib.dump(stacking_model, 'stacking_disease_model.pkl')\n",
    "print(\"   ✓ Modelo salvo: stacking_disease_model.pkl\")\n",
    "\n",
    "# Salvar label encoder\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"   ✓ Label encoder salvo: label_encoder.pkl\")\n",
    "\n",
    "# Salvar lista de features\n",
    "feature_names = X.columns.tolist()\n",
    "with open('feature_names.json', 'w') as f:\n",
    "    json.dump(feature_names, f)\n",
    "print(\"   ✓ Features salvas: feature_names.json\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. FUNÇÃO DE PREDIÇÃO COM JSON\n",
    "# ==============================================================================\n",
    "\n",
    "def predict_disease_from_json(json_input):\n",
    "    \"\"\"\n",
    "    Prediz doença a partir de um JSON de sintomas.\n",
    "    \n",
    "    Args:\n",
    "        json_input (dict ou str): JSON com sintomas (1 = presente, 0 = ausente)\n",
    "                                  Sintomas não fornecidos são considerados 0\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dicionário com predição e probabilidades\n",
    "    \"\"\"\n",
    "    # Carregar artefatos\n",
    "    model = joblib.load('stacking_disease_model.pkl')\n",
    "    encoder = joblib.load('label_encoder.pkl')\n",
    "    with open('feature_names.json', 'r') as f:\n",
    "        features = json.load(f)\n",
    "    \n",
    "    # Parse JSON se for string\n",
    "    if isinstance(json_input, str):\n",
    "        json_input = json.loads(json_input)\n",
    "    \n",
    "    # Criar vetor de features (default = 0)\n",
    "    feature_vector = np.zeros(len(features))\n",
    "    \n",
    "    # Preencher sintomas fornecidos\n",
    "    for symptom, value in json_input.items():\n",
    "        if symptom in features:\n",
    "            idx = features.index(symptom)\n",
    "            feature_vector[idx] = int(value)\n",
    "    \n",
    "    # Reshape para predição\n",
    "    feature_vector = feature_vector.reshape(1, -1)\n",
    "    \n",
    "    # Predição\n",
    "    prediction = model.predict(feature_vector)[0]\n",
    "    probabilities = model.predict_proba(feature_vector)[0]\n",
    "    \n",
    "    # Decodificar doença\n",
    "    disease = encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    # Top 5 doenças mais prováveis\n",
    "    top_5_indices = np.argsort(probabilities)[-5:][::-1]\n",
    "    top_5_diseases = [\n",
    "        {\n",
    "            'disease': encoder.inverse_transform([idx])[0],\n",
    "            'probability': float(probabilities[idx])\n",
    "        }\n",
    "        for idx in top_5_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'predicted_disease': disease,\n",
    "        'confidence': float(probabilities[prediction]),\n",
    "        'top_5_predictions': top_5_diseases\n",
    "    }\n",
    "\n",
    "print(\"\\n   ✓ Função de predição criada: predict_disease_from_json()\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. EXEMPLO DE USO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXEMPLO DE PREDIÇÃO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Exemplo de JSON com sintomas\n",
    "example_json = {\n",
    "    \"fever\": 1,\n",
    "    \"cough\": 1,\n",
    "    \"fatigue\": 1,\n",
    "    \"headache\": 1,\n",
    "    \"shortness of breath\": 1\n",
    "}\n",
    "\n",
    "print(\"\\nSintomas de entrada (JSON):\")\n",
    "print(json.dumps(example_json, indent=2))\n",
    "\n",
    "# Realizar predição\n",
    "result = predict_disease_from_json(example_json)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RESULTADO DA PREDIÇÃO:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\nDoença Prevista: {result['predicted_disease']}\")\n",
    "print(f\"Confiança: {result['confidence']:.4f} ({result['confidence']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTop 5 Diagnósticos Mais Prováveis:\")\n",
    "for i, pred in enumerate(result['top_5_predictions'], 1):\n",
    "    print(f\"   {i}. {pred['disease']}: {pred['probability']:.4f} ({pred['probability']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SISTEMA PRONTO PARA USO!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPara fazer novas predições, use:\")\n",
    "print(\"   result = predict_disease_from_json(seu_json)\")\n",
    "print(\"\\nExemplo de JSON vazio (todos sintomas = 0):\")\n",
    "print(\"   result = predict_disease_from_json({})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==============================================================================\n",
    "# INFORMAÇÕES SOBRE MEMÓRIA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n📊 Uso de Memória:\")\n",
    "print(f\"   - Tamanho do modelo em disco: ~{joblib.dump(stacking_model, 'temp.pkl')} bytes\")\n",
    "import os\n",
    "if os.path.exists('temp.pkl'):\n",
    "    size_mb = os.path.getsize('temp.pkl') / (1024 * 1024)\n",
    "    print(f\"   - Tamanho aproximado: {size_mb:.2f} MB\")\n",
    "    os.remove('temp.pkl')\n",
    "\n",
    "print(\"\\n✅ Sistema otimizado para uso consistente e eficiente de memória!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
