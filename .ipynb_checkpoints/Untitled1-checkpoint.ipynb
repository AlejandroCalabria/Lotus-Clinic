{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27855aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SISTEMA DE DIAGNÓSTICO MÉDICO - STACKING OTIMIZADO\n",
      "================================================================================\n",
      "\n",
      "[1/7] Carregando dataset...\n",
      "   ✓ Dataset: 246926 amostras, 378 colunas\n",
      "   ✓ Doenças únicas: 754\n",
      "   ✓ Sintomas: 377\n",
      "\n",
      "[2/7] Dividindo dados...\n",
      "   ✓ Treino: 197540 | Teste: 49386\n",
      "\n",
      "[3/7] Treinando Base Learner 1: Random Forest...\n",
      "   ✓ Random Forest treinado | Acurácia: 0.5166\n",
      "\n",
      "[4/7] Treinando Base Learner 2: Gradient Boosting...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAMENTO E PRÉ-PROCESSAMENTO DOS DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SISTEMA DE DIAGNÓSTICO MÉDICO - STACKING OTIMIZADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "print(\"\\n[1/7] Carregando dataset...\")\n",
    "df = pd.read_csv('Final_Augmented_dataset_Diseases_and_Symptoms.csv')\n",
    "# Remover doenças raras (ocorrência única)\n",
    "df = df[df['diseases'].isin(df['diseases'].value_counts()[df['diseases'].value_counts() > 1].index)]\n",
    "\n",
    "print(f\"   ✓ Dataset: {df.shape[0]} amostras, {df.shape[1]} colunas\")\n",
    "print(f\"   ✓ Doenças únicas: {df['diseases'].nunique()}\")\n",
    "print(f\"   ✓ Sintomas: {df.shape[1] - 1}\")\n",
    "\n",
    "# Separar features e target\n",
    "X = df.drop('diseases', axis=1)\n",
    "y = df['diseases']\n",
    "\n",
    "# Codificar labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DIVISÃO DOS DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[2/7] Dividindo dados...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"   ✓ Treino: {X_train.shape[0]} | Teste: {X_test.shape[0]}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. TREINAMENTO RÁPIDO DOS BASE LEARNERS (SEM CV)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[3/7] Treinando Base Learner 1: Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=50,  # Reduzido de 100\n",
    "    max_depth=15,     # Reduzido de 20\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_acc = accuracy_score(y_test, rf_model.predict(X_test))\n",
    "print(f\"   ✓ Random Forest treinado | Acurácia: {rf_acc:.4f}\")\n",
    "\n",
    "print(\"\\n[4/7] Treinando Base Learner 2: Gradient Boosting...\")\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=50,  # Reduzido de 100\n",
    "    max_depth=5,      # Reduzido de 10\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.7,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_acc = accuracy_score(y_test, gb_model.predict(X_test))\n",
    "print(f\"   ✓ Gradient Boosting treinado | Acurácia: {gb_acc:.4f}\")\n",
    "\n",
    "print(\"\\n[5/7] Treinando Base Learner 3: Naive Bayes...\")\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_acc = accuracy_score(y_test, nb_model.predict(X_test))\n",
    "print(f\"   ✓ Naive Bayes treinado | Acurácia: {nb_acc:.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CRIAR FEATURES PARA META-LEARNER (STACKING MANUAL)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[6/7] Criando features para Meta-Learner (Stacking)...\")\n",
    "\n",
    "# Predições dos base learners no conjunto de treino (usando probabilidades)\n",
    "rf_proba_train = rf_model.predict_proba(X_train)\n",
    "gb_proba_train = gb_model.predict_proba(X_train)\n",
    "nb_proba_train = nb_model.predict_proba(X_train)\n",
    "\n",
    "# Concatenar predições como novas features\n",
    "X_train_meta = np.hstack([rf_proba_train, gb_proba_train, nb_proba_train])\n",
    "\n",
    "# Mesmo processo para conjunto de teste\n",
    "rf_proba_test = rf_model.predict_proba(X_test)\n",
    "gb_proba_test = gb_model.predict_proba(X_test)\n",
    "nb_proba_test = nb_model.predict_proba(X_test)\n",
    "\n",
    "X_test_meta = np.hstack([rf_proba_test, gb_proba_test, nb_proba_test])\n",
    "\n",
    "print(f\"   ✓ Features meta criadas: {X_train_meta.shape[1]} colunas\")\n",
    "\n",
    "# Treinar meta-learner\n",
    "print(\"\\n   Treinando Meta-Learner (Logistic Regression)...\")\n",
    "meta_learner = LogisticRegression(\n",
    "    max_iter=500,  # Reduzido de 1000\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    solver='lbfgs',\n",
    "    verbose=0\n",
    ")\n",
    "meta_learner.fit(X_train_meta, y_train)\n",
    "print(\"   ✓ Meta-Learner treinado!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. AVALIAÇÃO DO MODELO STACKING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[7/7] Avaliando Stacking Ensemble...\")\n",
    "\n",
    "# Predições finais\n",
    "y_pred_train = meta_learner.predict(X_train_meta)\n",
    "y_pred_test = meta_learner.predict(X_test_meta)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\n   ✓ Acurácia TREINO: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"   ✓ Acurácia TESTE:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Relatório resumido\n",
    "report = classification_report(y_test, y_pred_test, output_dict=True, zero_division=0)\n",
    "print(f\"\\n   Métricas Médias (Weighted):\")\n",
    "print(f\"   - Precisão:  {report['weighted avg']['precision']:.4f}\")\n",
    "print(f\"   - Recall:    {report['weighted avg']['recall']:.4f}\")\n",
    "print(f\"   - F1-Score:  {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. SALVAR MODELOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SALVANDO MODELOS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Salvar todos os modelos\n",
    "joblib.dump(rf_model, 'rf_model.pkl')\n",
    "joblib.dump(gb_model, 'gb_model.pkl')\n",
    "joblib.dump(nb_model, 'nb_model.pkl')\n",
    "joblib.dump(meta_learner, 'meta_learner.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "# Salvar feature names\n",
    "feature_names = X.columns.tolist()\n",
    "with open('feature_names.json', 'w') as f:\n",
    "    json.dump(feature_names, f)\n",
    "\n",
    "print(\"   ✓ Modelos salvos:\")\n",
    "print(\"      - rf_model.pkl\")\n",
    "print(\"      - gb_model.pkl\")\n",
    "print(\"      - nb_model.pkl\")\n",
    "print(\"      - meta_learner.pkl\")\n",
    "print(\"      - label_encoder.pkl\")\n",
    "print(\"      - feature_names.json\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. FUNÇÃO DE PREDIÇÃO COM JSON\n",
    "# ==============================================================================\n",
    "\n",
    "def predict_disease_from_json(json_input):\n",
    "    \"\"\"\n",
    "    Prediz doença a partir de JSON de sintomas.\n",
    "    Sintomas não fornecidos são considerados 0 (falso).\n",
    "    \n",
    "    Args:\n",
    "        json_input (dict ou str): Sintomas {nome: valor}\n",
    "    \n",
    "    Returns:\n",
    "        dict: Predição, confiança e top 5 diagnósticos\n",
    "    \"\"\"\n",
    "    # Carregar modelos\n",
    "    rf = joblib.load('rf_model.pkl')\n",
    "    gb = joblib.load('gb_model.pkl')\n",
    "    nb = joblib.load('nb_model.pkl')\n",
    "    meta = joblib.load('meta_learner.pkl')\n",
    "    encoder = joblib.load('label_encoder.pkl')\n",
    "    \n",
    "    with open('feature_names.json', 'r') as f:\n",
    "        features = json.load(f)\n",
    "    \n",
    "    # Parse JSON\n",
    "    if isinstance(json_input, str):\n",
    "        json_input = json.loads(json_input)\n",
    "    \n",
    "    # Criar vetor de features (default = 0)\n",
    "    feature_vector = np.zeros(len(features))\n",
    "    \n",
    "    for symptom, value in json_input.items():\n",
    "        if symptom in features:\n",
    "            idx = features.index(symptom)\n",
    "            feature_vector[idx] = int(value)\n",
    "    \n",
    "    feature_vector = feature_vector.reshape(1, -1)\n",
    "    \n",
    "    # Predições dos base learners\n",
    "    rf_proba = rf.predict_proba(feature_vector)\n",
    "    gb_proba = gb.predict_proba(feature_vector)\n",
    "    nb_proba = nb.predict_proba(feature_vector)\n",
    "    \n",
    "    # Concatenar para meta-learner\n",
    "    meta_features = np.hstack([rf_proba, gb_proba, nb_proba])\n",
    "    \n",
    "    # Predição final\n",
    "    prediction = meta.predict(meta_features)[0]\n",
    "    probabilities = meta.predict_proba(meta_features)[0]\n",
    "    \n",
    "    disease = encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    # Top 5 doenças\n",
    "    top_5_indices = np.argsort(probabilities)[-5:][::-1]\n",
    "    top_5_diseases = [\n",
    "        {\n",
    "            'disease': encoder.inverse_transform([idx])[0],\n",
    "            'probability': float(probabilities[idx])\n",
    "        }\n",
    "        for idx in top_5_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'predicted_disease': disease,\n",
    "        'confidence': float(probabilities[prediction]),\n",
    "        'top_5_predictions': top_5_diseases\n",
    "    }\n",
    "\n",
    "print(\"\\n✅ Função criada: predict_disease_from_json()\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. EXEMPLO DE USO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXEMPLO DE PREDIÇÃO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "example_json = {\n",
    "    \"fever\": 1,\n",
    "    \"cough\": 1,\n",
    "    \"fatigue\": 1,\n",
    "    \"headache\": 1,\n",
    "    \"shortness of breath\": 1\n",
    "}\n",
    "\n",
    "print(\"\\nSintomas de entrada:\")\n",
    "print(json.dumps(example_json, indent=2))\n",
    "\n",
    "result = predict_disease_from_json(example_json)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RESULTADO:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\n🏥 Doença Prevista: {result['predicted_disease']}\")\n",
    "print(f\"📊 Confiança: {result['confidence']:.4f} ({result['confidence']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n📋 Top 5 Diagnósticos:\")\n",
    "for i, pred in enumerate(result['top_5_predictions'], 1):\n",
    "    print(f\"   {i}. {pred['disease']}: {pred['probability']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ SISTEMA PRONTO!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n💡 Para usar:\")\n",
    "print(\"   resultado = predict_disease_from_json({'febre': 1, 'tosse': 1})\")\n",
    "print(\"\\n⚡ Otimizado para velocidade e eficiência de memória!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
