{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9031c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SISTEMA DE DIAGN√ìSTICO M√âDICO - STACKING ULTRA-R√ÅPIDO\n",
      "================================================================================\n",
      "\n",
      "[1/7] Carregando dataset...\n",
      "   ‚úì Dataset: 246926 amostras, 378 colunas\n",
      "   ‚úì Doen√ßas: 754 | Sintomas: 377\n",
      "\n",
      "[2/7] Dividindo dados...\n",
      "   ‚úì Treino: 197,540 | Teste: 49,386\n",
      "\n",
      "[3/7] Treinando Base Learner 1: Random Forest...\n",
      "   ‚úì Conclu√≠do em 19.5s | Acur√°cia: 0.3916 (39.16%)\n",
      "\n",
      "[4/7] Treinando Base Learner 2: Extra Trees...\n",
      "   ‚úì Conclu√≠do em 18.5s | Acur√°cia: 0.3912 (39.12%)\n",
      "\n",
      "[5/7] Treinando Base Learner 3: Naive Bayes...\n",
      "   ‚úì Conclu√≠do em 256.6s | Acur√°cia: 0.8667 (86.67%)\n",
      "\n",
      "[6/7] Criando Stacking Ensemble...\n",
      "   ‚Üí Gerando predi√ß√µes dos base learners...\n",
      "   ‚Üí Features meta: 2,262 colunas\n",
      "   ‚Üí Treinando Meta-Learner (Logistic Regression)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAMENTO E PR√â-PROCESSAMENTO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SISTEMA DE DIAGN√ìSTICO M√âDICO - STACKING ULTRA-R√ÅPIDO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\n[1/7] Carregando dataset...\")\n",
    "df = pd.read_csv('Final_Augmented_dataset_Diseases_and_Symptoms.csv')\n",
    "# Remover doen√ßas raras (ocorr√™ncia √∫nica)\n",
    "df = df[df['diseases'].isin(df['diseases'].value_counts()[df['diseases'].value_counts() > 1].index)]\n",
    "\n",
    "print(f\"   ‚úì Dataset: {df.shape[0]} amostras, {df.shape[1]} colunas\")\n",
    "print(f\"   ‚úì Doen√ßas: {df['diseases'].nunique()} | Sintomas: {df.shape[1] - 1}\")\n",
    "\n",
    "# Separar features e target\n",
    "X = df.drop('diseases', axis=1).values.astype(np.int8)  # Otimiza√ß√£o de mem√≥ria\n",
    "y = df['diseases'].values\n",
    "feature_names = df.drop('diseases', axis=1).columns.tolist()\n",
    "\n",
    "# Codificar labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DIVIS√ÉO DOS DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[2/7] Dividindo dados...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Treino: {X_train.shape[0]:,} | Teste: {X_test.shape[0]:,}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. BASE LEARNER 1: RANDOM FOREST (PARALELO)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[3/7] Treinando Base Learner 1: Random Forest...\")\n",
    "t1 = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=30,      # Reduzido de 50\n",
    "    max_depth=12,         # Reduzido de 15\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    max_samples=0.7,      # Usa apenas 70% dos dados por √°rvore\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    warm_start=False\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_acc = accuracy_score(y_test, rf_model.predict(X_test))\n",
    "\n",
    "print(f\"   ‚úì Conclu√≠do em {time.time()-t1:.1f}s | Acur√°cia: {rf_acc:.4f} ({rf_acc*100:.2f}%)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. BASE LEARNER 2: EXTRA TREES (MUITO MAIS R√ÅPIDO QUE GB)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[4/7] Treinando Base Learner 2: Extra Trees...\")\n",
    "t2 = time.time()\n",
    "\n",
    "et_model = ExtraTreesClassifier(\n",
    "    n_estimators=30,      # Extra Trees √© mais r√°pido que Random Forest\n",
    "    max_depth=12,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    max_samples=0.7,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    bootstrap=True\n",
    ")\n",
    "et_model.fit(X_train, y_train)\n",
    "et_acc = accuracy_score(y_test, et_model.predict(X_test))\n",
    "\n",
    "print(f\"   ‚úì Conclu√≠do em {time.time()-t2:.1f}s | Acur√°cia: {et_acc:.4f} ({et_acc*100:.2f}%)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. BASE LEARNER 3: NAIVE BAYES (INSTANT√ÇNEO)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[5/7] Treinando Base Learner 3: Naive Bayes...\")\n",
    "t3 = time.time()\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_acc = accuracy_score(y_test, nb_model.predict(X_test))\n",
    "\n",
    "print(f\"   ‚úì Conclu√≠do em {time.time()-t3:.1f}s | Acur√°cia: {nb_acc:.4f} ({nb_acc*100:.2f}%)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. STACKING: META-LEARNER\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[6/7] Criando Stacking Ensemble...\")\n",
    "t4 = time.time()\n",
    "\n",
    "# Predi√ß√µes dos base learners (probabilidades)\n",
    "print(\"   ‚Üí Gerando predi√ß√µes dos base learners...\")\n",
    "rf_proba_train = rf_model.predict_proba(X_train)\n",
    "et_proba_train = et_model.predict_proba(X_train)\n",
    "nb_proba_train = nb_model.predict_proba(X_train)\n",
    "\n",
    "rf_proba_test = rf_model.predict_proba(X_test)\n",
    "et_proba_test = et_model.predict_proba(X_test)\n",
    "nb_proba_test = nb_model.predict_proba(X_test)\n",
    "\n",
    "# Concatenar como features para meta-learner\n",
    "X_train_meta = np.hstack([rf_proba_train, et_proba_train, nb_proba_train])\n",
    "X_test_meta = np.hstack([rf_proba_test, et_proba_test, nb_proba_test])\n",
    "\n",
    "print(f\"   ‚Üí Features meta: {X_train_meta.shape[1]:,} colunas\")\n",
    "\n",
    "# Treinar meta-learner\n",
    "print(\"   ‚Üí Treinando Meta-Learner (Logistic Regression)...\")\n",
    "meta_learner = LogisticRegression(\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    solver='lbfgs',\n",
    "    verbose=0\n",
    ")\n",
    "meta_learner.fit(X_train_meta, y_train)\n",
    "\n",
    "print(f\"   ‚úì Stacking conclu√≠do em {time.time()-t4:.1f}s\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. AVALIA√á√ÉO FINAL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[7/7] Avaliando Stacking Ensemble...\")\n",
    "\n",
    "y_pred_train = meta_learner.predict(X_train_meta)\n",
    "y_pred_test = meta_learner.predict(X_test_meta)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\n   üéØ ACUR√ÅCIA TREINO: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"   üéØ ACUR√ÅCIA TESTE:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "# M√©tricas detalhadas\n",
    "report = classification_report(y_test, y_pred_test, output_dict=True, zero_division=0)\n",
    "print(f\"\\n   üìä M√©tricas M√©dias (Weighted):\")\n",
    "print(f\"      Precis√£o:  {report['weighted avg']['precision']:.4f}\")\n",
    "print(f\"      Recall:    {report['weighted avg']['recall']:.4f}\")\n",
    "print(f\"      F1-Score:  {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n   ‚è±Ô∏è  Tempo total de treinamento: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. SALVAR MODELOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SALVANDO MODELOS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "joblib.dump(rf_model, 'rf_model.pkl')\n",
    "joblib.dump(et_model, 'et_model.pkl')\n",
    "joblib.dump(nb_model, 'nb_model.pkl')\n",
    "joblib.dump(meta_learner, 'meta_learner.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "with open('feature_names.json', 'w') as f:\n",
    "    json.dump(feature_names, f)\n",
    "\n",
    "print(\"   ‚úì Todos os modelos salvos!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. FUN√á√ÉO DE PREDI√á√ÉO\n",
    "# ==============================================================================\n",
    "\n",
    "def predict_disease_from_json(json_input):\n",
    "    \"\"\"\n",
    "    Prediz doen√ßa a partir de JSON de sintomas.\n",
    "    Sintomas n√£o fornecidos = 0 (falso).\n",
    "    \n",
    "    Args:\n",
    "        json_input: dict ou str com sintomas\n",
    "    \n",
    "    Returns:\n",
    "        dict com predi√ß√£o, confian√ßa e top 5\n",
    "    \"\"\"\n",
    "    # Carregar modelos\n",
    "    rf = joblib.load('rf_model.pkl')\n",
    "    et = joblib.load('et_model.pkl')\n",
    "    nb = joblib.load('nb_model.pkl')\n",
    "    meta = joblib.load('meta_learner.pkl')\n",
    "    encoder = joblib.load('label_encoder.pkl')\n",
    "    \n",
    "    with open('feature_names.json', 'r') as f:\n",
    "        features = json.load(f)\n",
    "    \n",
    "    # Parse JSON\n",
    "    if isinstance(json_input, str):\n",
    "        json_input = json.loads(json_input)\n",
    "    \n",
    "    # Criar vetor (default = 0)\n",
    "    feature_vector = np.zeros(len(features), dtype=np.int8)\n",
    "    \n",
    "    for symptom, value in json_input.items():\n",
    "        if symptom in features:\n",
    "            idx = features.index(symptom)\n",
    "            feature_vector[idx] = int(value)\n",
    "    \n",
    "    feature_vector = feature_vector.reshape(1, -1)\n",
    "    \n",
    "    # Predi√ß√µes base learners\n",
    "    rf_proba = rf.predict_proba(feature_vector)\n",
    "    et_proba = et.predict_proba(feature_vector)\n",
    "    nb_proba = nb.predict_proba(feature_vector)\n",
    "    \n",
    "    # Meta features\n",
    "    meta_features = np.hstack([rf_proba, et_proba, nb_proba])\n",
    "    \n",
    "    # Predi√ß√£o final\n",
    "    prediction = meta.predict(meta_features)[0]\n",
    "    probabilities = meta.predict_proba(meta_features)[0]\n",
    "    \n",
    "    disease = encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    # Top 5\n",
    "    top_5_indices = np.argsort(probabilities)[-5:][::-1]\n",
    "    top_5 = [\n",
    "        {\n",
    "            'disease': encoder.inverse_transform([idx])[0],\n",
    "            'probability': float(probabilities[idx])\n",
    "        }\n",
    "        for idx in top_5_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'predicted_disease': disease,\n",
    "        'confidence': float(probabilities[prediction]),\n",
    "        'top_5_predictions': top_5\n",
    "    }\n",
    "\n",
    "print(\"\\n‚úÖ Fun√ß√£o criada: predict_disease_from_json()\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. EXEMPLO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTE DE PREDI√á√ÉO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "example = {\n",
    "    \"fever\": 1,\n",
    "    \"cough\": 1,\n",
    "    \"fatigue\": 1,\n",
    "    \"headache\": 1,\n",
    "    \"shortness of breath\": 1\n",
    "}\n",
    "\n",
    "print(\"\\nüìù Sintomas:\")\n",
    "for s, v in example.items():\n",
    "    print(f\"   ‚Ä¢ {s}: {v}\")\n",
    "\n",
    "result = predict_disease_from_json(example)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üè• RESULTADO:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\nDoen√ßa Prevista: {result['predicted_disease']}\")\n",
    "print(f\"Confian√ßa: {result['confidence']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüìã Top 5 Diagn√≥sticos Mais Prov√°veis:\")\n",
    "for i, p in enumerate(result['top_5_predictions'], 1):\n",
    "    bar = \"‚ñà\" * int(p['probability'] * 50)\n",
    "    print(f\"{i}. {p['disease']}\")\n",
    "    print(f\"   {bar} {p['probability']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ SISTEMA PRONTO PARA USO!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüí° Uso:\")\n",
    "print(\"   resultado = predict_disease_from_json({'fever': 1, 'cough': 1})\")\n",
    "print(\"   resultado = predict_disease_from_json({})  # Todos sintomas = 0\")\n",
    "print(\"\\n‚ö° Modelos:\")\n",
    "print(\"   ‚Ä¢ Random Forest (30 √°rvores)\")\n",
    "print(\"   ‚Ä¢ Extra Trees (30 √°rvores) - Substitui Gradient Boosting\")\n",
    "print(\"   ‚Ä¢ Naive Bayes\")\n",
    "print(\"   ‚Ä¢ Meta-Learner: Logistic Regression\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
