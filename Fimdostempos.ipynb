{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SISTEMA DE DIAGNÓSTICO MÉDICO - STACKING ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "[1/6] Carregando dataset...\n",
      "   ✓ Dataset carregado: 246926 amostras, 378 colunas\n",
      "   ✓ Número de doenças únicas: 754\n",
      "   ✓ Número de sintomas: 377\n",
      "\n",
      "   Classes de doenças codificadas: 0 a 753\n",
      "\n",
      "[2/6] Dividindo dados em treino e teste...\n",
      "   ✓ Treino: 197540 amostras\n",
      "   ✓ Teste: 49386 amostras\n",
      "\n",
      "[3/6] Configurando Stacking Ensemble...\n",
      "   ✓ Base Learners:\n",
      "      - Random Forest (100 árvores)\n",
      "      - Gradient Boosting (100 árvores)\n",
      "      - Naive Bayes\n",
      "   ✓ Meta Learner: Logistic Regression\n",
      "   ✓ Cross-Validation: 5-fold Stratified\n",
      "\n",
      "[4/6] Treinando modelo Stacking...\n",
      "   (Isso pode levar alguns minutos...)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAMENTO E PRÉ-PROCESSAMENTO DOS DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SISTEMA DE DIAGNÓSTICO MÉDICO - STACKING ENSEMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar dataset\n",
    "print(\"\\n[1/6] Carregando dataset...\")\n",
    "df = pd.read_csv('Final_Augmented_dataset_Diseases_and_Symptoms.csv')\n",
    "# Remover doenças raras (ocorrência única)\n",
    "df = df[df['diseases'].isin(df['diseases'].value_counts()[df['diseases'].value_counts() > 1].index)]\n",
    "\n",
    "\n",
    "print(f\"   ✓ Dataset carregado: {df.shape[0]} amostras, {df.shape[1]} colunas\")\n",
    "print(f\"   ✓ Número de doenças únicas: {df['diseases'].nunique()}\")\n",
    "print(f\"   ✓ Número de sintomas: {df.shape[1] - 1}\")\n",
    "\n",
    "# Separar features e target\n",
    "X = df.drop('diseases', axis=1)\n",
    "y = df['diseases']\n",
    "\n",
    "# Codificar labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\n   Classes de doenças codificadas: 0 a {len(label_encoder.classes_) - 1}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DIVISÃO DOS DADOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[2/6] Dividindo dados em treino e teste...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"   ✓ Treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"   ✓ Teste: {X_test.shape[0]} amostras\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DEFINIÇÃO DO MODELO STACKING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[3/6] Configurando Stacking Ensemble...\")\n",
    "\n",
    "# Base learners (modelos de primeira camada)\n",
    "base_learners = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )),\n",
    "    ('gb', GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "\n",
    "# Meta learner (modelo de segunda camada)\n",
    "meta_learner = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "# Criar Stacking Classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"   ✓ Base Learners:\")\n",
    "print(\"      - Random Forest (100 árvores)\")\n",
    "print(\"      - Gradient Boosting (100 árvores)\")\n",
    "print(\"      - Naive Bayes\")\n",
    "print(\"   ✓ Meta Learner: Logistic Regression\")\n",
    "print(\"   ✓ Cross-Validation: 5-fold Stratified\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TREINAMENTO DO MODELO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[4/6] Treinando modelo Stacking...\")\n",
    "print(\"   (Isso pode levar alguns minutos...)\\n\")\n",
    "\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n   ✓ Treinamento concluído!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. AVALIAÇÃO DO MODELO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[5/6] Avaliando modelo...\")\n",
    "\n",
    "# Predições\n",
    "y_pred_train = stacking_model.predict(X_train)\n",
    "y_pred_test = stacking_model.predict(X_test)\n",
    "\n",
    "# Acurácia\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\n   ✓ Acurácia no treino: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"   ✓ Acurácia no teste:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Relatório de classificação (resumido)\n",
    "print(\"\\n   Relatório de Classificação (Teste):\")\n",
    "print(\"   \" + \"-\" * 76)\n",
    "report = classification_report(y_test, y_pred_test, \n",
    "                              target_names=label_encoder.classes_,\n",
    "                              output_dict=True)\n",
    "\n",
    "# Mostrar métricas médias\n",
    "print(f\"   Precisão média:  {report['weighted avg']['precision']:.4f}\")\n",
    "print(f\"   Recall médio:    {report['weighted avg']['recall']:.4f}\")\n",
    "print(f\"   F1-Score médio:  {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. SALVAR MODELO E ARTEFATOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[6/6] Salvando modelo e artefatos...\")\n",
    "\n",
    "# Salvar modelo\n",
    "joblib.dump(stacking_model, 'stacking_disease_model.pkl')\n",
    "print(\"   ✓ Modelo salvo: stacking_disease_model.pkl\")\n",
    "\n",
    "# Salvar label encoder\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"   ✓ Label encoder salvo: label_encoder.pkl\")\n",
    "\n",
    "# Salvar lista de features\n",
    "feature_names = X.columns.tolist()\n",
    "with open('feature_names.json', 'w') as f:\n",
    "    json.dump(feature_names, f)\n",
    "print(\"   ✓ Features salvas: feature_names.json\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. FUNÇÃO DE PREDIÇÃO COM JSON\n",
    "# ==============================================================================\n",
    "\n",
    "def predict_disease_from_json(json_input):\n",
    "    \"\"\"\n",
    "    Prediz doença a partir de um JSON de sintomas.\n",
    "    \n",
    "    Args:\n",
    "        json_input (dict ou str): JSON com sintomas (1 = presente, 0 = ausente)\n",
    "                                  Sintomas não fornecidos são considerados 0\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dicionário com predição e probabilidades\n",
    "    \"\"\"\n",
    "    # Carregar artefatos\n",
    "    model = joblib.load('stacking_disease_model.pkl')\n",
    "    encoder = joblib.load('label_encoder.pkl')\n",
    "    with open('feature_names.json', 'r') as f:\n",
    "        features = json.load(f)\n",
    "    \n",
    "    # Parse JSON se for string\n",
    "    if isinstance(json_input, str):\n",
    "        json_input = json.loads(json_input)\n",
    "    \n",
    "    # Criar vetor de features (default = 0)\n",
    "    feature_vector = np.zeros(len(features))\n",
    "    \n",
    "    # Preencher sintomas fornecidos\n",
    "    for symptom, value in json_input.items():\n",
    "        if symptom in features:\n",
    "            idx = features.index(symptom)\n",
    "            feature_vector[idx] = int(value)\n",
    "    \n",
    "    # Reshape para predição\n",
    "    feature_vector = feature_vector.reshape(1, -1)\n",
    "    \n",
    "    # Predição\n",
    "    prediction = model.predict(feature_vector)[0]\n",
    "    probabilities = model.predict_proba(feature_vector)[0]\n",
    "    \n",
    "    # Decodificar doença\n",
    "    disease = encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    # Top 5 doenças mais prováveis\n",
    "    top_5_indices = np.argsort(probabilities)[-5:][::-1]\n",
    "    top_5_diseases = [\n",
    "        {\n",
    "            'disease': encoder.inverse_transform([idx])[0],\n",
    "            'probability': float(probabilities[idx])\n",
    "        }\n",
    "        for idx in top_5_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'predicted_disease': disease,\n",
    "        'confidence': float(probabilities[prediction]),\n",
    "        'top_5_predictions': top_5_diseases\n",
    "    }\n",
    "\n",
    "print(\"\\n   ✓ Função de predição criada: predict_disease_from_json()\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. EXEMPLO DE USO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXEMPLO DE PREDIÇÃO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Exemplo de JSON com sintomas\n",
    "example_json = {\n",
    "    \"fever\": 1,\n",
    "    \"cough\": 1,\n",
    "    \"fatigue\": 1,\n",
    "    \"headache\": 1,\n",
    "    \"shortness of breath\": 1\n",
    "}\n",
    "\n",
    "print(\"\\nSintomas de entrada (JSON):\")\n",
    "print(json.dumps(example_json, indent=2))\n",
    "\n",
    "# Realizar predição\n",
    "result = predict_disease_from_json(example_json)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RESULTADO DA PREDIÇÃO:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\nDoença Prevista: {result['predicted_disease']}\")\n",
    "print(f\"Confiança: {result['confidence']:.4f} ({result['confidence']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTop 5 Diagnósticos Mais Prováveis:\")\n",
    "for i, pred in enumerate(result['top_5_predictions'], 1):\n",
    "    print(f\"   {i}. {pred['disease']}: {pred['probability']:.4f} ({pred['probability']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SISTEMA PRONTO PARA USO!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPara fazer novas predições, use:\")\n",
    "print(\"   result = predict_disease_from_json(seu_json)\")\n",
    "print(\"\\nExemplo de JSON vazio (todos sintomas = 0):\")\n",
    "print(\"   result = predict_disease_from_json({})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==============================================================================\n",
    "# INFORMAÇÕES SOBRE MEMÓRIA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n📊 Uso de Memória:\")\n",
    "print(f\"   - Tamanho do modelo em disco: ~{joblib.dump(stacking_model, 'temp.pkl')} bytes\")\n",
    "import os\n",
    "if os.path.exists('temp.pkl'):\n",
    "    size_mb = os.path.getsize('temp.pkl') / (1024 * 1024)\n",
    "    print(f\"   - Tamanho aproximado: {size_mb:.2f} MB\")\n",
    "    os.remove('temp.pkl')\n",
    "\n",
    "print(\"\\n✅ Sistema otimizado para uso consistente e eficiente de memória!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d83ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes válidas: 754\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'squared_hinge', 'huber', 'modified_huber', 'perceptron', 'squared_error', 'epsilon_insensitive', 'hinge', 'squared_epsilon_insensitive'}. Got 'log' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     y_chunk_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(y_chunk)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Treinamento incremental\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     model\u001b[38;5;241m.\u001b[39mpartial_fit(X_chunk, y_chunk_encoded, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(encoder\u001b[38;5;241m.\u001b[39mclasses_)))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 4️⃣ Avaliar\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Para avaliação, você pode usar um pedaço separado do CSV (ou 1 chunk)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m test_chunk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)  \u001b[38;5;66;03m# exemplo de teste rápido\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1140\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1141\u001b[0m )\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    630\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    640\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    641\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'squared_hinge', 'huber', 'modified_huber', 'perceptron', 'squared_error', 'epsilon_insensitive', 'hinge', 'squared_epsilon_insensitive'}. Got 'log' instead."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
